---
title: "Identifying Causal Deck-Construction Effects in High-Dimensional Draft Environments"
author: "Demetrius DiMucci"
date: today
format:
  html:
    toc: true
    toc-depth: 3
    code-fold: true
---

## abstract

Booster draft is a play mode of many trading card games where players select cards from a limited pool and construct a deck to later compete with. Deck strength in limited formats emerges from three forces: a player's baseline win propensity (their skill), the composition of the deck they draft, and randomness in gameplay. these forces are entangled through the causal structure\
`W ← P → D → W`,\
where `P` is player skill, `D` is deck composition, and `W` is win outcome.

this article develops a method to identify the causal effect of deck composition on win rate in a high-dimensional setting using historical draft data for Magic: The Gathering (≈400 card features, ≈90k draft logs), despite strong confounding by skill. the approach combines hierarchical shrinkage, per-player baseline win estimation, high-dimensional modeling of deck performance, and simulation-based calibration. results show that naive regressions overstate the value of synergistic cards for highly skilled players, while the corrected estimator recovers a stable "deck bump" effect attributable to deck construction alone.

------------------------------------------------------------------------

## 1. introduction

predictive models for limited formats (e.g., xgboost, neural networks) estimate how good a deck *will* perform. they do not isolate *why*. in games like magic: the gathering, draft decisions couple tightly to player skill: stronger players select higher-value cards, commit earlier, navigate signals better, and construct smoother curves. this creates a persistent confounding problem:

-   deck composition `D` correlates with skill `P`,\
-   `P` also causes better win results `W`,\
-   so naïve models inflate the apparent value of cards favored by strong players.

i treat the draft ecosystem as a high-dimensional causal inference problem. the goal is to estimate\
`E[W | do(D)]`\
rather than `E[W | D]`, i.e., isolate the effect of deck construction itself.

------------------------------------------------------------------------

## 2. causal structure

### 2.1 dag

``` text
P ─────▶ W
│        ▲
│        │
└──────▶ D ─────▶ W
```

### 2.2 interpretation

-   **P (player skill)** affects both deck strength and win rate.\
-   **D (deck features)** is high-dimensional (\~400 card features and counts).\
-   **W (match outcome)** is binomial (best-of-3 events summarized to match winrate).

### 2.3 estimand

the target is the causal effect:

$$
\tau = E[W \mid do(D = d_1)] - E[W \mid do(D = d_0)].
$$

because intervention on deck composition is counterfactual, we recover this via baseline-skill adjustment.

------------------------------------------------------------------------

## 3. baseline win propensity

I estimate per-player baseline skill via a hierarchical beta-binomial model across historical drafts:

$$
W_i \sim \text{Binomial}(n_i, p_i), \\
p_i \sim \text{Beta}(\alpha,\beta).
$$

this shrinks noisy players toward the population mean while giving stable posteriors for grinders. the result is a **baseline win probability** $p_{\text{base}, i}$ for each player that approximates the $P \to W$ path.

------------------------------------------------------------------------

## 4. modeling deck-driven win probability

### 4.1 high-dimensional deck model

i build a model

$$
\hat{p}_{\text{deck}} = f(D),
$$

where $D$ is the deck composition after construction (mana curve, color identity, card counts, rares, removal density, synergy flags, diminishing-returns interactions, etc.). xgboost handles sparsity, nonlinearity, and interactions.

cross-validation is grouped by player id to avoid leakage of skill.

### 4.2 deck bump

the deck's causal contribution is operationalized as:

$$
\text{deck\_bump} = f(D) - p_{\text{base}}.
$$

this removes the direct effect of $P \to W$ and isolates the $D \to W$ path.\
in quasi-random drafts, residual confounding is small.

------------------------------------------------------------------------

## 5. Identifiability and simulation tests

#### A. What would perfect prediction would look like under the 7--3 stop rule?

Before discussing any fitted model, it is useful to ask: If we actually knew the truth how good would our predictions about the deck be? Assume we know, for each deck, its true match win probability ( p ) and precisely how the underlying components contribute to it. Even in this "oracle" setting, a draft tournament run does not show us ( p ) directly. Instead, each deck is run through a truncated tournament where the run is stopped after accumulating either 7 wins or 3 losses. We can observe only the final record and from that compute an empirical win-rate for the deck and we can approximate the skill level of the player piloting the deck. This stopping rule injects noise. A perfect predictive model cannot remove this noise; it can only sit underneath it. Then the key question of interest is given these limitations on observations is how close to truth about net deck effect and bump can we get?

To study this, I built a generative simulator that creates a synthetic draft ecosystem with full control over the \"true\" underlying quantities. The simulator is intentionally simple, only five cards exist but it captures the essential causal structure. Each simulated player has a latent skill value which determines their baseline win probability $p$. The player's underlying skill influences which cards the player adds to their decks, real players don't pick cards at random and vary in their abilities to assess the quality of cards. Each card has an intrinsic likelihood of being selected and the probability of selection is modified by the player's skill. High skill players will select win-rate increasing cards more often than low skill players. This allows the simulation to inject confounding of the type $P$ $\rightarrow$ $D$ $\rightarrow$ $W$, $P$ $\rightarrow$ $W$ that we see in real draft data.

The deck composition then produces a true deck effect. Once the draft is simulated the deck's composition is run through a structural function that generates a true "deck bump", this can be interpreted as a modifier to the player's baseline win probability. This function considers the individual card strengths, interactions and anti-synergies between subsets of cards, skill-dependent effects (e.g. some complicated card may be misplayed frequently by low skill players), and an upper bound on how much a deck can add or subtract from the baseline skill. These elements produce the true deck effect $\Delta p_{deck}$ .

The baseline skill and the true deck effect combine to create the true match win-rate probability: $p_{true} = logistic(skill) + \Delta p_{deck}$

In a real draft this value would be invisible.

Next, the Magic Arena 7W - 3L best-of-one tournament logic is applied. Each deck is run through the same truncated process that characterizes the Premier Draft structure. At each stage of the tournament the outcome of the match is determined by $p_{true}$ and stopped when either 7 wins are reached or 3 losses. A simplifying assumption i make is that $p_{true}$ is constant throughout the run, in reality this is not the case due to Arena's underlying matchmaking algorithm. This process produces an observed win rate, what we would see in practice when analyzing real draft data. Our observed deck bump would then be:\
$\Delta p_{obs} = \hat p_{MLE} - p_{baseline}$

Now that we know the ground truth we can ask "How much does the stopping rule distort the relationship between $\Delta p_{true}$ and $\Delta p_{obs}$?". "What fraction of the variance is irreducible noise?". "What is the best possible $R^2$ any model could ever hope to achieve?" These quantities define the upper bound on model performance.

### findings

1.  naïve regressions overstate effect sizes when $\text{cov}(P, D)$ is large.\
2.  the baseline-adjusted estimator recovers the true deck effect across a wide range of confounding strengths.\
3.  high dimensionality (200--500 features) does not break identification when:
    -   shrinkage is applied to rare card counts,\
    -   interactions are modeled flexibly,\
    -   model training is grouped by player.

::: cell
```{r}
# placeholder: insert simulation plotting code here
# e.g., ggplot(sim_results, aes(x = true_effect, y = est_effect, color = method)) + ...
```
:::

------------------------------------------------------------------------

## 6. empirical results on 17lands FIN format

### 6.1 data

-   \~90,000 drafts\
-   \~400 card features\
-   complete event histories\
-   multiple replications per player

### 6.2 calibration

posterior predictions for $f(D)$ are nearly perfectly calibrated after beta-binomial smoothing. plotting $f(D)$ vs empirical win rate gives slope \~1.0 (vs \~1.3 naïve).

::: cell
```{r}
# placeholder for calibration curve
# ggplot(calib_df, aes(pred_bin, empirical_rate)) + ...
```
:::

### 6.3 player-strength correction

card-level marginal value curves differ substantially after accounting for $P$. for example:

-   high-synergy cards appear overpowered in naïve models because skilled players draft them disproportionately.\
-   after correction, the value curve flattens, revealing the actual diminishing returns.

(insert sahagin figure here when ready.)

### 6.4 per-card marginal curves

for each card $c$:

$$
m_{c}(n) = f(D_{c}=n) - f(D_{c}=n-1),
$$

revealing synergy pockets and diminishing returns without skill-induced distortion.

------------------------------------------------------------------------

## 7. interpretability

i compute:

-   model-agnostic SHAP values for card contributions,\
-   synergy matrices using pairwise deltas,\
-   archetype embeddings based on deck-bump profiles.

together, these reveal:

-   which archetypes benefit most from high-skill pilots,\
-   which cards have intrinsic value vs "skill halo" distortion,\
-   how diminishing returns align with gameplay heuristics.

------------------------------------------------------------------------

## 8. discussion

the broader contribution is methodological: deck construction is a reproducible sandbox for causal inference with:

-   high-dimensional treatments $D$,\
-   structured confounding via $P$,\
-   large sample sizes,\
-   quasi-random assignment noise from packs.

this same causal template appears in biological data:

-   $P$ = genomic background or copy number,\
-   $D$ = high-dimensional gene expression,\
-   $W$ = drug response.

the deck-bump estimator parallels expression-mediated effect isolation.

------------------------------------------------------------------------

## 9. limitations

-   late-draft picks are not fully random.\
-   synergy leakage into the baseline estimate can occur for player specialists.\
-   cross-set generalization is not guaranteed.

------------------------------------------------------------------------

## 10. conclusion

isolating $D \to W$ in high-dimensional decision systems is feasible when skill is explicitly modeled and posterior adjustments isolate the structural contribution of deck composition. this produces interpretable, stable estimates of card and deck value and serves as a general causal-inference template.

------------------------------------------------------------------------

## references

-   pearl, j. *causality*.\
-   imbens, g., rubin, d. *causal inference for statistics, social, and biomedical sciences*.\
-   chen, t., guestrin, c. (2016). xgboost: a scalable tree boosting system.\
-   17lands.com performance and draft log data.
